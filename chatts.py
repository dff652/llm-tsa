import math
from typing import List, Tuple, Optional
import re

import time 
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    BitsAndBytesConfig,
)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from utils import ts_downsample, plot_ts_with_anomalies, extract_anomalies, map_anomalies_to_original

class ChatTSAnalyzer:
    """
    ChatTS-14B æ—¶åºåˆ†æå™¨ï¼ˆæ¨ç†ç‰ˆï¼‰
    - å•å¡æ¨ç†ï¼ˆæ¨è 4-bit é‡åŒ–ä»¥æ§åˆ¶æ˜¾å­˜ï¼‰
    - æ”¯æŒé•¿åºåˆ—æŒ‰æ»‘çª—æ¨ç†å¹¶åˆå¹¶
    - ä»…ä½¿ç”¨æ¨¡å‹æ”¯æŒçš„ç”Ÿæˆå‚æ•°ï¼ˆå»æ‰ temperature/top_kï¼‰

    ä¾èµ–:
      pip install "transformers>=4.43" accelerate bitsandbytes torch --extra-index-url https://download.pytorch.org/whl/cu118
    """

    def __init__(
        self,
        model_path: str,
        device: str = "cuda:0",
        load_in_4bit: bool = True,
        attn_implementation: str = "eager",   # 2080Ti/V100 ç­‰ pre-Ampere å»ºè®®ç”¨ eager
        torch_dtype: torch.dtype = torch.float16,
    ):
        """
        Args:
            model_path: æœ¬åœ°æˆ–HFè·¯å¾„ï¼Œæ¯”å¦‚ "/home/data1/llm_models/bytedance-research/ChatTS-14B"
            device:    ç»Ÿä¸€æ”¾åˆ°åŒä¸€å¼ å¡ä¸Šï¼Œä¾‹å¦‚ "cuda:0"
            load_in_4bit: æ˜¯å¦ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–
            attn_implementation: 'eager' / 'sdpa' / 'flash_attention_2'ï¼ˆè€å¡ç”¨ 'eager'ï¼‰
            torch_dtype: å»ºè®® fp16ï¼ˆ2080Ti ä¸æ”¯æŒ bfloat16ï¼‰
        """
        self.model_path = model_path
        self.device = torch.device(device)
        self.compute_dtype = torch_dtype  # ä¿å­˜è®¡ç®— dtype ç”¨äºè¾“å…¥è½¬æ¢
        
        m = re.match(r"cuda:(\d+)", device)
        device_index = int(m.group(1)) if m else 0

        bnb_config = None
        if load_in_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch_dtype,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            
            # 4bit æƒ…å†µï¼šç›´æ¥è®© HF æŠŠæ¨¡å‹æ”¾åˆ°æŒ‡å®šå¡ä¸Šï¼Œåˆ«å† .to()
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                quantization_config=bnb_config,
                low_cpu_mem_usage=True,
                attn_implementation=attn_implementation,
                device_map={"": device_index},   # ğŸ‘ˆ æ•´æ¨¡å‹åœ¨æŒ‡å®š index ä¸Š
            )
        else:
            # éé‡åŒ–ï¼šæ­£å¸¸ from_pretrained + .to()
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                quantization_config=bnb_config,
                low_cpu_mem_usage=True,
                attn_implementation=attn_implementation,
            ).to(self.device)
        

        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.processor = AutoProcessor.from_pretrained(
            model_path, trust_remote_code=True, tokenizer=self.tokenizer
        )

        # æŸäº›åˆ†æ”¯å¯èƒ½æ²¡æœ‰ pad_token_idï¼Œå…œåº•åˆ° eos
        if getattr(self.model.config, "pad_token_id", None) is None:
            self.model.config.pad_token_id = self.model.config.eos_token_id

    # ------------------
    # å†…éƒ¨å·¥å…·
    # ------------------
    def _build_prompt(
        self,
        timeseries_len: int,
        system_prompt: str,
        task_prompt_tpl: str,
    ) -> str:
        user_prompt = task_prompt_tpl.format(ts_len=timeseries_len)
        prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>"
            f"<|im_start|>user\n{user_prompt}<|im_end|><|im_start|>assistant\n"
        )
        return prompt

    def _prepare_inputs(
        self,
        prompt: str,
        timeseries: np.ndarray,
    ):
        # processor ä¼šä¸º ChatTS åŒæ—¶å¤„ç† text & timeseries
        inputs = self.processor(
            text=[prompt],
            timeseries=[timeseries],
            padding=True,
            return_tensors="pt",
        )

        # ä½¿ç”¨åˆå§‹åŒ–æ—¶ä¿å­˜çš„è®¡ç®— dtype
        # å¯¹äºé‡åŒ–æ¨¡å‹ï¼Œè¿™æ˜¯ bnb_4bit_compute_dtypeï¼›å¯¹äºéé‡åŒ–æ¨¡å‹ï¼Œè¿™æ˜¯ torch_dtype
        model_dtype = self.compute_dtype

        # æŠŠæ‰€æœ‰å¼ é‡ç§»åˆ°åŒä¸€è®¾å¤‡ï¼Œå¹¶ä¸”ï¼š
        #    - åªå¯¹"æµ®ç‚¹å¼ é‡"è½¬æ¢ dtypeï¼ˆå¦‚ timeseries ç›¸å…³çš„å¼ é‡ï¼‰
        #    - ä¿ç•™ input_ids / attention_mask è¿™äº›æ•´å‹ä¸åŠ¨
        for k, v in inputs.items():
            if torch.is_tensor(v):
                v = v.to(self.device)
                if v.is_floating_point():
                    v = v.to(model_dtype)
                inputs[k] = v

        return inputs

    

    def _generate(
        self,
        inputs,
        max_new_tokens: int = 1024,
        top_p: float = 0.9,
    ) -> str:
        """
        ä»…ä¼ é€’æ¨¡å‹æ”¯æŒçš„ç”Ÿæˆå‚æ•°ï¼š
          - å»æ‰ temperatureã€top_kï¼ˆä¹‹å‰å·²è¢«æ¨¡å‹å¿½ç•¥å¹¶è­¦å‘Šï¼‰
          - åªä¿ç•™ top_p & do_sample/use_cache/max_new_tokens
        """
        gen_kwargs = dict(
            max_new_tokens=max_new_tokens,
            use_cache=True,
        )
        # å½“ top_p < 1.0 æ—¶å¯ç”¨é‡‡æ ·ï¼›=1.0 æ—¶èµ°è´ªå¿ƒ
        if top_p < 1.0:
            gen_kwargs.update(dict(do_sample=True, top_p=top_p))
        else:
            gen_kwargs.update(dict(do_sample=False))

        with torch.inference_mode():
            outputs = self.model.generate(**inputs, **gen_kwargs)

        # åˆ‡é™¤å‰ç¼€ prompt tokensï¼Œå¾—åˆ°å¹²å‡€å›ç­”
        text = self.tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True
        )
        return text.strip()

    def _run_one_window(
        self,
        timeseries: np.ndarray,
        max_new_tokens: int,
        top_p: float,
        system_prompt: str,
        task_prompt_tpl: str,
    ) -> str:
        prompt = self._build_prompt(
            timeseries_len=len(timeseries),
            system_prompt=system_prompt,
            task_prompt_tpl=task_prompt_tpl,
        )
        inputs = self._prepare_inputs(prompt, timeseries)
        return self._generate(inputs, max_new_tokens=max_new_tokens, top_p=top_p)

    @staticmethod
    def _make_windows(
        n: int,
        window_len: int,
        overlap: float,
    ) -> List[Tuple[int, int]]:
        """
        è¿”å›ä¸€ç»„ [start, end) ç´¢å¼•çª—å£ã€‚
        overlap: 0 ~ <1 ï¼Œä¾‹å¦‚ 0.25 è¡¨ç¤ºæ¯çª—é‡å  25%
        """
        assert 0 <= overlap < 1, "overlap éœ€åœ¨ [0,1) ä¹‹é—´"
        if n <= window_len:
            return [(0, n)]
        stride = max(1, int(window_len * (1 - overlap)))
        starts = list(range(0, max(1, n - window_len + 1), stride))
        if starts[-1] + window_len < n:
            starts.append(n - window_len)
        return [(s, min(n, s + window_len)) for s in starts]

    # ------------------
    # å¯¹å¤–ä¸»å…¥å£
    # ------------------
    def analyze(
        self,
        timeseries: np.ndarray,
        max_new_tokens: int = 1024,
        window_len: Optional[int] = None,
        overlap: float = 0.25,
        per_window_new_tokens: Optional[int] = None,
        top_p: float = 1,
        system_prompt: str = "You are a helpful assistant.",
        task_prompt_tpl: str = (
            "I have a time series length of {ts_len}: <ts><ts/>. "
            "Please analyze the local changes in this time series."
        ),
        clear_cuda_cache_each_window: bool = False,
        header_each_window: bool = True,
    ) -> str:
        """
        Args:
            timeseries: ä¸€ç»´ numpy æ•°ç»„
            max_new_tokens: å•çª—/æ•´æ®µçš„æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆè¿‡å¤§å®¹æ˜“ OOMï¼Œå»ºè®® 512~2048ï¼‰
            window_len: è‹¥ä¸º None æˆ– len(ts) <= window_lenï¼Œåˆ™æ•´æ®µæ¨ç†ï¼›å¦åˆ™æ»‘çª—
            overlap: æ»‘çª—é‡å æ¯”ä¾‹ï¼ˆ0~<1ï¼‰
            per_window_new_tokens: æ¯ä¸ªçª—å£å•ç‹¬çš„ max_new_tokensï¼ˆé»˜è®¤è‡ªåŠ¨æŒ‰æ€»ä¸Šé™åˆ†é…ï¼‰
            top_p: nucleus samplingï¼›=1 æ—¶ä¸é‡‡æ ·ï¼ˆè´ªå¿ƒï¼‰
            clear_cuda_cache_each_window: æ¯çª—åæ¸…ç†ç¼“å­˜ä»¥å‡å°‘ç¢ç‰‡
            header_each_window: è¾“å‡ºé‡Œç»™æ¯ä¸ªçª—åŠ ä¸€ä¸ªå¤´éƒ¨è¡Œï¼Œæ ‡æ³¨åŒºé—´ä¸åºå·
        """
        assert timeseries.ndim == 1, "timeseries éœ€è¦æ˜¯ä¸€ç»´æ•°ç»„"

        # æƒ…å†µ1ï¼šæ•´æ®µç›´æ¥è·‘
        if window_len is None or len(timeseries) <= window_len:
            return self._run_one_window(
                timeseries=timeseries,
                max_new_tokens=max_new_tokens,
                top_p=top_p,
                system_prompt=system_prompt,
                task_prompt_tpl=task_prompt_tpl,
            )

        # æƒ…å†µ2ï¼šæ»‘çª—
        windows = self._make_windows(n=len(timeseries), window_len=window_len, overlap=overlap)
        num_windows = len(windows)

        # é»˜è®¤ç»™æ¯ä¸ªçª—åˆ†é…ä¸€ä¸ªè¾ƒåˆç†çš„è¾“å‡ºä¸Šé™
        pnt = per_window_new_tokens or max(
            128, min(1024, max_new_tokens // max(1, num_windows))
        )

        pieces: List[str] = []
        for i, (s, e) in enumerate(windows, 1):
            seg = timeseries[s:e]
            try:
                txt = self._run_one_window(
                    timeseries=seg,
                    max_new_tokens=pnt,
                    top_p=top_p,
                    system_prompt=system_prompt,
                    task_prompt_tpl=task_prompt_tpl,
                )
            except torch.cuda.OutOfMemoryError:
                # ç®€å•çš„é€€é¿ç­–ç•¥ï¼šå°è¯•æŠŠè¯¥çª—çš„max_new_tokenså‡åŠå†è·‘ä¸€æ¬¡
                torch.cuda.empty_cache()
                fallback_tokens = max(64, pnt // 2)
                txt = self._run_one_window(
                    timeseries=seg,
                    max_new_tokens=fallback_tokens,
                    top_p=top_p,
                    system_prompt=system_prompt,
                    task_prompt_tpl=task_prompt_tpl,
                )

            if header_each_window:
                pieces.append(f"[Window {i}/{num_windows}: {s}-{e}]\n{txt}")
            else:
                pieces.append(txt)

            if clear_cuda_cache_each_window:
                torch.cuda.empty_cache()

        # ç®€å•æ‹¼æ¥ï¼ˆéœ€è¦æ›´å¼ºåˆå¹¶å¯å†åŠ â€œä¸€æ¬¡æ€»ç»“åˆå¹¶â€æ­¥éª¤ï¼‰
        return "\n\n".join(pieces)

# ------------------
# ä½¿ç”¨ç¤ºä¾‹
# ------------------
if __name__ == "__main__":
    import os

    # å¯é€‰ï¼šå‡å°‘ç¢ç‰‡ï¼ˆæ–°ç‰ˆæœ¬ PyTorch æ”¯æŒï¼‰
    # os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

    # å‡†å¤‡æ•°æ®
    # ts = np.random.randn(400_000).astype(np.float32)
    
    # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†è·¯å¾„ï¼Œé¿å…å·¥ä½œç›®å½•é—®é¢˜
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, 'Data', 'PI_20412.PV.csv')
    df = pd.read_csv(csv_path, index_col=0)
    
    n =768
    downsampled_data, time_index, position_index = ts_downsample(df['PI_20412.PV'],  n_out=n)
    downsampled_ts = downsampled_data.values
        
    analyzer = ChatTSAnalyzer(
        model_path="/home/data1/llm_models/bytedance-research/ChatTS-14B",
        device="cuda:1",
        load_in_4bit=True,           # 22GB æ˜¾å­˜å»ºè®® True
        attn_implementation="eager", # 2080Ti å»ºè®® eager
        # torch_dtype=torch.float16,
        torch_dtype=torch.bfloat16
    )
    
    prompt = (
    "æˆ‘æœ‰ä¸€ä¸ªé•¿åº¦ä¸º {ts_len} çš„æ—¶é—´åºåˆ—ï¼š<ts><ts/>ã€‚"
    "è¯·è¯†åˆ«è¯¥æ—¶é—´åºåˆ—ä¸­æ‰€æœ‰å¼‚å¸¸æˆ–å¼‚å¸¸ç‰‡æ®µã€‚"
    "å¯¹äºæ¯ä¸ªå¼‚å¸¸ï¼Œè¯·æè¿°ä»¥ä¸‹å†…å®¹ï¼š\n"
    "- å¼‚å¸¸å‘ç”Ÿçš„ç´¢å¼•åŒºé—´ï¼ˆé—­åŒºé—´ï¼Œèµ·æ­¢ç´¢å¼•å‡ä¸ºæ•´æ•°ï¼‰\n"
    "- åç¦»çš„å¹…åº¦æˆ–æ¨¡å¼ç‰¹å¾ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰\n"
    "- å¯èƒ½çš„åŸå› ï¼ˆä¾‹å¦‚ï¼šçªç„¶è·³å˜ã€è¶‹åŠ¿æ¼‚ç§»ã€å¼‚å¸¸ç‚¹ã€å™ªå£°çªå¢ç­‰ï¼‰\n"
    "ä»å…¨å±€æ•°æ®çš„è§†è§’æ‰¾å‡ºå…·æœ‰æ˜æ˜¾ç»Ÿè®¡æ˜¾è‘—æ€§çš„å¼‚å¸¸ï¼ˆä¾‹å¦‚æ¥è¿‘ 0 çš„æç«¯å€¼ï¼‰ï¼Œå¿½ç•¥æ­£å¸¸çš„å‘¨æœŸæ€§æ³¢åŠ¨ã€‚\n"
    "\n"
    "è¾“å‡ºè¦æ±‚ï¼šä»…è¾“å‡ºä¸€ä¸ªåä¸º anomalies çš„ JSON æ•°ç»„ï¼ˆä¸è¦è¾“å‡ºå…¶å®ƒæ–‡å­—ã€ä¸è¦åŠ ä»£ç å—æ ‡è®°ï¼‰ä¸¥æ ¼æŒ‰ç…§ç¤ºä¾‹æ ¼å¼,å¿…é¡»ç”¨ä¸­æ–‡å›å¤ã€‚"
    "æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ åŒ…å«å­—æ®µï¼šrangeï¼ˆå½¢å¦‚ [start, end] çš„æ•°ç»„ï¼‰ã€ampï¼ˆæ•°å€¼ï¼‰ã€labelï¼ˆå­—ç¬¦ä¸²ï¼‰ã€detailï¼ˆå­—ç¬¦ä¸²ï¼‰ã€"
    "colorï¼ˆå­—ç¬¦ä¸²ï¼‰ã€extremeï¼ˆå¯é€‰ï¼Œ'min'|'max'|'auto'ï¼‰ã€‚\n"
    "\n"
    "ç¤ºä¾‹ï¼ˆä»…ä½œæ ¼å¼å‚è€ƒï¼Œä¸è¦ç…§æŠ„æ•°å€¼ï¼‰ï¼š\n"
    "anomalies = [\n"
    "    {{\n"
    "        \"range\": [137, 139],\n"
    "        \"amp\": 1.91,\n"
    "        \"label\": \"Downward spike\",\n"
    "        \"detail\": \"Drops from ~1.91 to ~0.00 then recovers; possible transient interference or system failure.\",\n"
    "        \"color\": \"red\",\n"
    "        \"extreme\": \"min\"\n"
    "    }}\n"
    "]\n"
    )
    
    st = time.time()
    text = analyzer.analyze(downsampled_ts, 
                            max_new_tokens=1024,
                            top_p=1,
                            task_prompt_tpl = prompt)
    et = time.time()
    print(et-st)
    print(text)
    
    anomalies = extract_anomalies(text)
    print(len(anomalies), anomalies[:1])
    
    # å°†å¼‚å¸¸ç´¢å¼•æ˜ å°„åˆ°åŸå§‹æ•°æ®ï¼ˆä½¿ç”¨ position_indexï¼‰
    mapped_anomalies = map_anomalies_to_original(anomalies, position_index)
    print("æ˜ å°„åçš„å¼‚å¸¸ï¼ˆåŸå§‹æ•°æ®ç´¢å¼•ï¼‰:", mapped_anomalies[:1])
    
    # ä½¿ç”¨åŸå§‹æ•°æ®å’Œæ˜ å°„åçš„å¼‚å¸¸è¿›è¡Œç»˜å›¾
    original_ts = df['PI_20412.PV'].values
    fig, ax, notes = plot_ts_with_anomalies(
        original_ts, mapped_anomalies,
        number_style="plain",      # â‘  â‘¡ â‘¢â€¦
        notes_loc="bottom",         # è¯´æ˜æ”¾åœ¨ä¸‹æ–¹ï¼ˆä¸é®æŒ¡å›¾ï¼‰
        marker_fontsize=22
    )
    print(notes)  # å¦‚æœä½ ä¹Ÿæƒ³åœ¨æ§åˆ¶å°æ‰“å°å‡ºæ¥
    results_dir = os.path.join(script_dir, 'Results')
    os.makedirs(results_dir, exist_ok=True)  # ç¡®ä¿ Results ç›®å½•å­˜åœ¨
    plt.savefig(os.path.join(results_dir, 'ChatTS_anomalies.png'))

    # 1) çŸ­åºåˆ—/èƒ½æ”¾ä¸‹ï¼šæ•´æ®µæ¨ç†
    # text = analyzer.analyze(ts, max_new_tokens=1024)

    # 2) é•¿åºåˆ—ï¼šæ»‘çª—æ¨ç†
    # text = analyzer.analyze(
    #     ts,
    #     window_len=50_000,            # æ ¹æ®æ˜¾å­˜/ååè‡ªå·±è°ƒ
    #     overlap=0.25,
    #     max_new_tokens=2000,         # æ€»ä¸Šé™
    #     per_window_new_tokens=256,   # æ¯çª—ä¸Šé™ï¼ˆä¸ä¼ åˆ™è‡ªåŠ¨å‡åˆ†ï¼‰
    #     top_p=0.9,
    #     clear_cuda_cache_each_window=True,
    # )
    # print(text[:2000], "...\n[TRUNCATED]")